# app/RSS.py (ä¿®å¤æ—¶åŒºç‰ˆ)
import feedparser
from curl_cffi import requests as cffi_requests
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
from datetime import datetime, timedelta, timezone  # [æ–°å¢] å¼•å…¥ timezone

# å°è¯•å¯¼å…¥ trafilatura
try:
    import trafilatura
    HAS_TRAFILATURA = True
except ImportError:
    HAS_TRAFILATURA = False

# Jina Reader
JINA_PREFIX = "https://r.jina.ai/"

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
}

def clean_text(text):
    """æ¸…æ´—æ–‡æœ¬ï¼Œå»é™¤å¤šä½™ç©ºè¡Œ"""
    if not text: return ""
    lines = [line.strip() for line in text.split('\n') if line.strip()]
    return '\n'.join(lines)

def fetch_content_local(url, timeout=15):
    """æœ¬åœ°ç›´æ¥æŠ“å–"""
    try:
        resp = cffi_requests.get(url, headers=HEADERS, impersonate="chrome110", timeout=timeout, allow_redirects=True)
        if resp.status_code != 200: return None

        if HAS_TRAFILATURA:
            text = trafilatura.extract(resp.content, include_comments=False)
            if text: return clean_text(text)
        
        soup = BeautifulSoup(resp.content, "html.parser")
        for tag in soup(['script', 'style', 'nav', 'footer', 'header', 'iframe', 'aside']):
            tag.decompose()
        
        article = soup.find('article')
        if article: return clean_text(article.get_text(separator='\n'))
        
        ps = soup.find_all('p')
        text = '\n'.join([p.get_text() for p in ps if len(p.get_text()) > 20])
        return clean_text(text)
    except Exception as e:
        print(f"æœ¬åœ°æŠ“å–å¤±è´¥ {url}: {e}")
        return None

def fetch_content_jina(url):
    """Jina å…œåº•"""
    try:
        target = f"{JINA_PREFIX}{url}"
        headers = HEADERS.copy()
        headers['x-respond-with'] = 'markdown'
        resp = cffi_requests.get(target, headers=headers, impersonate="chrome110", timeout=20)
        if resp.status_code == 200 and len(resp.text) > 100:
            return clean_text(resp.text.replace("Input the URL to scrape another page.", ""))
    except: pass
    return None

def fetch_url_smart(url):
    """æ™ºèƒ½è·¯ç”±: æœ¬åœ° -> Jina"""
    content = fetch_content_local(url)
    if content and len(content) > 100: return content
    print(f"ğŸ”„ æœ¬åœ°å¤±è´¥ï¼Œå°è¯• Jina: {url[:50]}...")
    return fetch_content_jina(url)

def fetch_rss_content(rss_url, hours_limit=12, max_items_safety=50, max_length=5000):
    """
    æŠ“å– RSS å¹¶æå–å†…å®¹ (å·²ä¿®å¤æ—¶åŒºé—®é¢˜)
    """
    print(f"ğŸš€ å¼€å§‹ä»»åŠ¡: {rss_url}")
    try:
        # 1. è·å– RSS
        try:
            rss_resp = cffi_requests.get(rss_url, headers=HEADERS, impersonate="chrome110", timeout=15)
            feed = feedparser.parse(rss_resp.content)
        except:
            rss_text = fetch_content_jina(rss_url)
            feed = feedparser.parse(rss_text)

        if not feed.entries:
            return "RSSæºä¸ºç©ºæˆ–æ— æ³•è®¿é—®ã€‚"

        # 2. æ—¶é—´ç­›é€‰ (ç»Ÿä¸€ä½¿ç”¨ UTC)
        now = datetime.now(timezone.utc) # [ä¿®æ­£] è·å–å½“å‰ UTC æ—¶é—´
        target_entries = []
        
        # print(f"ğŸ•’ [æ—¶é—´åŸºå‡†] UTC Now: {now}") # è°ƒè¯•ç”¨

        for entry in feed.entries:
            pub_date = None
            try:
                # [ä¿®æ­£] ç›´æ¥å°† struct_time è½¬æ¢ä¸º UTC aware datetime
                if hasattr(entry, 'published_parsed') and entry.published_parsed:
                    pub_date = datetime(*entry.published_parsed[:6], tzinfo=timezone.utc)
                elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                    pub_date = datetime(*entry.updated_parsed[:6], tzinfo=timezone.utc)
            except Exception:
                pub_date = None
            
            title = entry.get('title', 'æ— æ ‡é¢˜')
            
            if pub_date:
                diff = now - pub_date
                # ç­›é€‰é€»è¾‘
                if diff < timedelta(hours=hours_limit) and diff > timedelta(days=-1): # é˜²æ­¢æœªæ¥æ—¶é—´(è¯¯æŠ¥)
                    target_entries.append(entry)
                    print(f"  âœ… é‡‡çº³: {title[:15]}... | è·ä»Š: {diff}")
                else:
                    # print(f"  âŒ ä¸¢å¼ƒ(è¶…æ—¶): {title[:15]}... | è·ä»Š: {diff}")
                    pass
            else:
                # æ— æ—¶é—´æˆ³çš„ä¿åº•ç­–ç•¥ï¼šåªå–å‰5æ¡ï¼Œé˜²æ­¢å…¨æ˜¯æ—§æ–‡
                if len(target_entries) < 5:
                    target_entries.append(entry)
                    print(f"  âš ï¸ é‡‡çº³(æ— æ—¶é—´): {title[:15]}...")

        if not target_entries:
            print("âš ï¸ è­¦å‘Š: æ²¡æœ‰ç¬¦åˆæ—¶é—´èŒƒå›´çš„æ–‡ç« ï¼Œå°†å¼ºåˆ¶ä½¿ç”¨æœ€æ–°çš„ 3 ç¯‡ã€‚")
            target_entries = feed.entries[:3]
        
        # 3. æ•°é‡æˆªæ–­
        target_entries = target_entries[:max_items_safety]
        print(f"ğŸ“Š æœ€ç»ˆå‘½ä¸­ {len(target_entries)} ç¯‡ï¼Œå¼€å§‹å¹¶å‘æŠ“å–...")

        # 4. å¹¶å‘æŠ“å–æ­£æ–‡
        results_map = {}
        with ThreadPoolExecutor(max_workers=8) as executor:
            future_to_idx = {
                executor.submit(fetch_url_smart, entry.link): i 
                for i, entry in enumerate(target_entries)
            }
            
            for future in as_completed(future_to_idx):
                idx = future_to_idx[future]
                results_map[idx] = future.result()

        # 5. ç»„è£…ç»“æœ
        context = f"ã€æƒ…æŠ¥æ±‡æ€»ã€‘(æ¥æº: {feed.feed.get('title', 'RSS')})\n\n"
        success_c = 0
        for i, entry in enumerate(target_entries):
            title = entry.get('title', 'æ— æ ‡é¢˜')
            link = entry.get('link', '')
            content = results_map.get(i)
            
            if not content:
                desc = entry.get('summary', '') or entry.get('description', '')
                try: clean_desc = BeautifulSoup(desc, "html.parser").get_text(strip=True)
                except: clean_desc = desc[:200]
                content = f"[æŠ“å–å¤±è´¥] æ‘˜è¦: {clean_desc}"
            else:
                success_c += 1
                if len(content) > max_length:
                    content = content[:max_length] + "..."

            context += f"=== {i+1}. {title} ===\né“¾æ¥: {link}\nå‘å¸ƒæ—¶é—´: {entry.get('published', 'æœªçŸ¥')}\nå†…å®¹:\n{content}\n\n"
            
        print(f"ğŸ å®Œæˆ: {success_c}/{len(target_entries)}")
        return context

    except Exception as e:
        return f"ç³»ç»Ÿé”™è¯¯: {e}"
